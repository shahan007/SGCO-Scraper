{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "floppy-talent",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h1 style=\"text-align: center;color:Blue;\">\n",
    "    Scraper for SGCO\n",
    "</h1>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-constitution",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h3 style=\"color:darkorange;\">\n",
    "    Note!    \n",
    "<h3>\n",
    "<p>\n",
    "    for xpath expressions try placing the output from bs4 in <a target=\"_blank\" href=\"https://scrapinghub.github.io/xpath-playground/\">Xpath Playground </a>    \n",
    "</p>\n",
    "<br><br>    \n",
    "<p>Execute the following command:</p>   \n",
    "<ul style=\"color:green;\">            \n",
    "    <li>soup = BeautifulSoup(response.text)</li>\n",
    "    <li>soup.select(\"#table24\")</li>\n",
    "</ul>\n",
    "<p>\n",
    "    during scraping there seems to be no tbody tag implemented but the browsers inserts it as it creates its own dom tree.\n",
    "</p>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-movie",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "active-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mandatory\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from lxml import html\n",
    "import random\n",
    "# optional\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-basics",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Main Script</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unable-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotateHeader:\n",
    "\n",
    "    DEFAULT_USER_AGENT_LIST=[\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/44.0.2403.155 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; OpenBSD i386) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1944.0 Safari/537.36\"\n",
    "    ]\n",
    "    \n",
    "        \n",
    "    #get hostname from chrome developers tool; execute --> window.location.hostname\n",
    "    REQUEST_HOST_NAME = \"singapore-companies-directory.com\"    \n",
    "    HEADERS = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "        \"Accept-Encoding\": \"gzip, deflate\", \n",
    "        \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\", \n",
    "        \"Dnt\": \"1\", \n",
    "        \"Host\": REQUEST_HOST_NAME, \n",
    "        \"Upgrade-Insecure-Requests\": \"1\", \n",
    "        \"User-Agent\": \"\", \n",
    "    }\n",
    "    \n",
    "    def _generate_random_default_header(self):\n",
    "        return random.choice(type(self).DEFAULT_USER_AGENT_LIST)\n",
    "            \n",
    "    \n",
    "    def fetch_header(self):\n",
    "        header =  type(self).HEADERS\n",
    "        header[\"User-Agent\"] = self._generate_random_default_header()\n",
    "        return header\n",
    "\n",
    "# ------ Helper Methods ------\n",
    "def generate_prod_serv_names(xpath_data,expression):\n",
    "    \"\"\"\n",
    "    Generate Products and Service Names. It takes in expression for each type and fetched the cleaned data\n",
    "    \"\"\"\n",
    "    \n",
    "    regex_strip_expression = r'[\\n\\r\\t\\xa0]'\n",
    "    regex_strip_rtn = re.compile(regex_strip_expression)\n",
    "    xpath_data_name = xpath_data.xpath(expression)\n",
    "    xpath_data_name = [regex_strip_rtn.sub('', str_).strip() for str_ in xpath_data_name]\n",
    "    return xpath_data_name\n",
    "\n",
    "def zipper(links,names):\n",
    "    \"\"\"\n",
    "    Zips relative links and link associated text name\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(zip(names,links))\n",
    "\n",
    "def convert_to_absolute_links(linkArray,base_url):\n",
    "    \"\"\"\n",
    "    Transform the relative links to absolute links\n",
    "    \"\"\"\n",
    "    \n",
    "    return [req.compat.urljoin(base_url, link) for link in linkArray]\n",
    "\n",
    "\n",
    "def fetch_prod_serv_links(rotate_header):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetches the name and absolute links of all the available categories of Singapore Listed Companies\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = \"http://singapore-companies-directory.com\"\n",
    "    main_url = \"http://singapore-companies-directory.com/sitemap.htm\"\n",
    "    headers = rotate_header.fetch_header()\n",
    "    res = req.get(main_url, headers=headers)\n",
    "    tree = html.fromstring(html=res.text)\n",
    "    expression = \"//*[@id='table16']/tr[not(position()=3)]\"\n",
    "    browser = \"//*[@id='table16']//tr[not(position()=3)]\"\n",
    "    products,services = tree.xpath(expression)\n",
    "    products_categories_links = convert_to_absolute_links(products.xpath(\".//p//a/@href\"),base_url)\n",
    "    services_categories_links = convert_to_absolute_links(services.xpath(\"./td[2]//a/@href\"),base_url)\n",
    "    products_categories_names = generate_prod_serv_names(products,\".//p//a/font/text()\")\n",
    "    services_categories_names = generate_prod_serv_names(services,\"./td[2]//a//font/text()\")\n",
    "    productLinkName = zipper(products_categories_links,products_categories_names)\n",
    "    serviceLinkName = zipper(services_categories_links,services_categories_names)\n",
    "    return productLinkName,serviceLinkName\n",
    "\n",
    "def fetch_category_inst_links(base_url, main_url,rotate_header):\n",
    "    \"\"\"\n",
    "    Fetches Absolute links for each category link\n",
    "    \"\"\"\n",
    "    res = req.get(main_url, headers=rotate_header.fetch_header())\n",
    "    if res.ok:\n",
    "        tree = html.fromstring(html=res.text)\n",
    "        # you can runs the expression below, only works under browser inspector tool\n",
    "        browser_express = \"//table[@id='table24']/tbody/tr[7]//table/tbody/tr[not(position()=1 ) and not(position()=last())]//a/font/text()\"\n",
    "        # main expression to scrape all the  institutes links; we skip the first two tr as they were blanks; returns relative links\n",
    "        script_expression = \"//table[@id='table24']/tr[7]//table/tr[not(position()=1 ) and not(position()=last())]//a/@href\"\n",
    "        inst_links = tree.xpath(script_expression)\n",
    "        # converts each relative link to absolute so we can follow them\n",
    "        absol_links = convert_to_absolute_links(inst_links,base_url)\n",
    "        return absol_links\n",
    "    return None\n",
    "\n",
    "def scrape_data(link_array, headers):\n",
    "    \"\"\"\n",
    "    Follow the absolute links provided and scrape the data\n",
    "    \"\"\"\n",
    "\n",
    "    for link in link_array:\n",
    "        res = req.get(link, headers=headers)\n",
    "        tree = html.fromstring(html=res.text)\n",
    "        expression = \"(//table//table//table)[8]//tr/td[1]/descendant-or-self::text()\"\n",
    "        company_data = tree.xpath(expression)\n",
    "        company_data_str = \" \".join(company_data)\n",
    "\n",
    "        # ----regx-----\n",
    "        # compile so can use multiple timesstrip all the blank stuff\n",
    "        regex_strip_expression = r'[\\n\\r\\t\\xa0]'\n",
    "        regex_strip_rtn = re.compile(regex_strip_expression)\n",
    "        # ----date cleaning-----\n",
    "        cleaned_data = regex_strip_rtn.sub('', company_data_str).strip()\n",
    "        data_needed = ['Contact', 'Address', \"Tel\", \"Fax\",\n",
    "                       \"e-mail\", \"Website\", \"Categories\", \"Company Profile\"]\n",
    "        unavail = []\n",
    "        data_needed_index = []\n",
    "        final_data = {}\n",
    "        for need in data_needed:\n",
    "            try:\n",
    "                data_needed_index.append(\n",
    "                    cleaned_data.lower().index(need.lower()))\n",
    "            except:\n",
    "                unavail.append(need)\n",
    "        for unavaildata in unavail:\n",
    "            data_needed.remove(unavaildata)\n",
    "        for i in range(len(data_needed_index)-1):\n",
    "            start_index = data_needed_index[i]\n",
    "            stop_index = data_needed_index[i+1]\n",
    "            final_data[data_needed[i]] = \"\".join(\n",
    "                cleaned_data[start_index:stop_index].split(':')[1:]).strip() or None\n",
    "        try:\n",
    "            final_data[data_needed[-1]] = cleaned_data[data_needed_index[-1]:].split(':')[1].strip() or None\n",
    "            final_data['Company'] = cleaned_data[:data_needed_index[0]].strip() or None\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            final_data['e-mail'] = final_data['e-mail'].split()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            categories = final_data['Categories'].split(',')\n",
    "            categories = list(map(lambda cat: cat.strip(), categories))\n",
    "            if categories == [\"\"]:\n",
    "                categories = None\n",
    "            final_data['Categories'] = categories\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            website_slash_index = final_data['Website'].index(\"//www\")\n",
    "            final_data['Website'] = final_data['Website'][:website_slash_index] + \\\n",
    "                \":\" + final_data['Website'][website_slash_index:]\n",
    "        except:\n",
    "            pass\n",
    "        yield final_data\n",
    "        \n",
    "def scrape_type(array_type,array,base_url,main_data,rotate_header):\n",
    "    \n",
    "    if array_type == \"Products\":\n",
    "        index = 0\n",
    "    else:\n",
    "        index = 1\n",
    "    \n",
    "    print(f\"\\n\\nScraping Type: {array_type}\\n\\n\")\n",
    "    for array_name, array_link in array:\n",
    "        print(f\"\\n\\n\\tScraping Cat: {array_name}\\n\\n\")\n",
    "        institute_links = fetch_category_inst_links(base_url,array_link,rotate_header)\n",
    "        if not institute_links:\n",
    "            continue       \n",
    "        local_data = {array_name:[]}\n",
    "        for inst_data in scrape_data(institute_links,rotate_header.fetch_header()):\n",
    "            local_data[array_name].append(inst_data)                                        \n",
    "        main_data[index][array_type].append(local_data)\n",
    "    print(f\"\\n\\nScraping Type: {array_type} Done !\\n\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main Function\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting Scraping: Please wait ! Don't quit !\\n\\n\")\n",
    "\n",
    "    # setting constant variables\n",
    "    base_url = \"http://singapore-companies-directory.com\"     \n",
    "    rotate_header =  RotateHeader()\n",
    "    main_data = [\n",
    "        {\n",
    "            \"Products\" : []\n",
    "        },\n",
    "        {\n",
    "            \"Services\" : []\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    productLinkName,serviceLinkName = fetch_prod_serv_links(rotate_header)\n",
    "    scrape_type(\"Products\",productLinkName,base_url,main_data,rotate_header)\n",
    "    scrape_type(\"Services\",serviceLinkName,base_url,main_data,rotate_header)\n",
    "    baseDir = os.path.abspath(os.path.dirname(__name__))\n",
    "    filename = 'DataOutput/data.json'\n",
    "    filePath = os.path.join(baseDir, filename)\n",
    "    with open(filePath, mode='w') as outfile:\n",
    "        json.dump(main_data, outfile, indent=4)\n",
    "    print(f\"Scraping Done! Checkout the data in the following file:{filename}\\n\\n\")   \n",
    "    return main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "requested-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------Utility functions----------------\n",
    "def read_data(filename='./DataOutput/data.json'):\n",
    "    baseDir = os.path.abspath(os.path.dirname(__file__))\n",
    "    filePath = os.path.join(baseDir, filename)\n",
    "    with open(filePath, mode='r') as outfile:\n",
    "        return json.load(outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
